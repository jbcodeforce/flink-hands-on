# -----------------------------------------------------------------------------
# Confluent Cloud Infrastructure
# Shared Infrastructure for Flink Projects
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Environment (conditional: create new or use existing)
# -----------------------------------------------------------------------------
# Data source for existing environment by ID
data "confluent_environment" "cdc_env_by_id" {
  count = var.existing_environment_id != null ? 1 : 0
  id    = var.existing_environment_id
}

# Data source for existing environment by name (try to find existing)
# If this succeeds, we'll use the existing environment instead of creating a new one
data "confluent_environment" "cdc_env_by_name" {
  count = var.existing_environment_id == null && var.existing_environment_name != null ? 1 : 0
  display_name = var.existing_environment_name
}

# Create new environment with random ID (only if no ID or name provided)
resource "confluent_environment" "cdc_env" {
  count = var.existing_environment_id == null && var.existing_environment_name == null ? 1 : 0

  display_name = "${var.prefix}-environment-${random_id.env_display_id.hex}"

  stream_governance {
    package = "ADVANCED"
  }

  lifecycle {
    prevent_destroy = false
  }
}

# Create new environment with specified name (if name is provided)
# Uses the name directly without random_id suffix
# Note: If an environment with this name already exists, use existing_environment_id instead
# or the data source above will find it and we'll use that ID
resource "confluent_environment" "cdc_env_with_name" {
  count = var.existing_environment_id == null && var.existing_environment_name != null ? 1 : 0

  display_name = var.existing_environment_name

  stream_governance {
    package = "ADVANCED"
  }

  lifecycle {
    prevent_destroy = false
  }
}

# Local values to reference resources (created or existing)
# Priority: existing ID > existing name (data source) > created with name > created with random ID
locals {
  environment_id = var.existing_environment_id != null ? var.existing_environment_id : (
    var.existing_environment_name != null ? (
      # Try to use data source if it found existing environment, otherwise use created resource
      # Note: If data source errors (env doesn't exist), Terraform will fail and user should handle
      # by either removing the variable or using existing_environment_id if the env exists
      length(data.confluent_environment.cdc_env_by_name) > 0 && data.confluent_environment.cdc_env_by_name[0].id != null ? data.confluent_environment.cdc_env_by_name[0].id : confluent_environment.cdc_env_with_name[0].id
    ) : confluent_environment.cdc_env[0].id
  )
  environment_resource_name = var.existing_environment_id != null ? data.confluent_environment.cdc_env_by_id[0].resource_name : (
    var.existing_environment_name != null ? (
      length(data.confluent_environment.cdc_env_by_name) > 0 && data.confluent_environment.cdc_env_by_name[0].resource_name != null ? data.confluent_environment.cdc_env_by_name[0].resource_name : confluent_environment.cdc_env_with_name[0].resource_name
    ) : confluent_environment.cdc_env[0].resource_name
  )
}

# -----------------------------------------------------------------------------
# Kafka Cluster (conditional: create new or use existing)
# -----------------------------------------------------------------------------
resource "confluent_kafka_cluster" "cdc_cluster" {
  count = var.existing_kafka_cluster_id == null ? 1 : 0

  display_name = "${var.prefix}-cluster-${random_id.env_display_id.hex}"
  availability = var.cc_availability
  cloud        = "AWS"
  region       = var.cloud_region

  standard {}

  environment {
    id = local.environment_id
  }

  lifecycle {
    prevent_destroy = false
  }
}

# Data source for existing Kafka cluster
data "confluent_kafka_cluster" "cdc_cluster" {
  count = var.existing_kafka_cluster_id != null ? 1 : 0
  id    = var.existing_kafka_cluster_id

  environment {
    id = local.environment_id
  }
}

# Additional local values
locals {
  kafka_cluster_id = var.existing_kafka_cluster_id != null ? var.existing_kafka_cluster_id : confluent_kafka_cluster.cdc_cluster[0].id
  kafka_cluster_rest_endpoint = var.existing_kafka_cluster_id != null ? data.confluent_kafka_cluster.cdc_cluster[0].rest_endpoint : confluent_kafka_cluster.cdc_cluster[0].rest_endpoint
}

# -----------------------------------------------------------------------------
# Schema Registry (conditional: use existing or auto-provisioned)
# -----------------------------------------------------------------------------
# Data source for existing Schema Registry by ID
data "confluent_schema_registry_cluster" "cdc_sr_by_id" {
  count = var.existing_schema_registry_id != null ? 1 : 0
  id    = var.existing_schema_registry_id

  environment {
    id = local.environment_id
  }
}

# Data source for auto-provisioned Schema Registry
data "confluent_schema_registry_cluster" "cdc_sr_auto" {
  count = var.existing_schema_registry_id == null ? 1 : 0
  environment {
    id = local.environment_id
  }

  depends_on = [
    confluent_kafka_cluster.cdc_cluster,
    data.confluent_kafka_cluster.cdc_cluster
  ]
}

# Local value to reference Schema Registry (existing or auto-provisioned)
locals {
  schema_registry_id = var.existing_schema_registry_id != null ? var.existing_schema_registry_id : data.confluent_schema_registry_cluster.cdc_sr_auto[0].id
  schema_registry_endpoint = var.existing_schema_registry_id != null ? data.confluent_schema_registry_cluster.cdc_sr_by_id[0].rest_endpoint : data.confluent_schema_registry_cluster.cdc_sr_auto[0].rest_endpoint
  schema_registry = var.existing_schema_registry_id != null ? data.confluent_schema_registry_cluster.cdc_sr_by_id[0] : data.confluent_schema_registry_cluster.cdc_sr_auto[0]
}



# -----------------------------------------------------------------------------
# Flink Compute Pool (conditional: create new or use existing)
# -----------------------------------------------------------------------------
resource "confluent_flink_compute_pool" "cdc_compute_pool" {
  count = var.existing_flink_compute_pool_id == null ? 1 : 0

  display_name = "${var.prefix}-compute-pool-${random_id.env_display_id.hex}"
  cloud        = "AWS"
  region       = var.cloud_region
  max_cfu      = var.flink_max_cfu

  environment {
    id = local.environment_id
  }

  lifecycle {
    prevent_destroy = false
  }
}

# Data source for existing Flink compute pool
data "confluent_flink_compute_pool" "cdc_compute_pool" {
  count = var.existing_flink_compute_pool_id != null ? 1 : 0
  id    = var.existing_flink_compute_pool_id

  environment {
    id = local.environment_id
  }
}

# Local value to reference Flink compute pool (created or existing)
locals {
  flink_compute_pool_id = var.existing_flink_compute_pool_id != null ? var.existing_flink_compute_pool_id : confluent_flink_compute_pool.cdc_compute_pool[0].id
  flink_compute_pool = var.existing_flink_compute_pool_id != null ? data.confluent_flink_compute_pool.cdc_compute_pool[0] : confluent_flink_compute_pool.cdc_compute_pool[0]
}

# -----------------------------------------------------------------------------
# Flink Region Data Source
# -----------------------------------------------------------------------------
# Flink API keys are scoped to a Flink region, not a compute pool
data "confluent_flink_region" "cdc_flink_region" {
  cloud  = "AWS"
  region = var.cloud_region
}

# -----------------------------------------------------------------------------
# Service Account (conditional: create new or use existing)
# -----------------------------------------------------------------------------
resource "confluent_service_account" "cdc_sa" {
  count = var.existing_service_account_id == null ? 1 : 0

  display_name = "${var.prefix}-service-account-${random_id.env_display_id.hex}"
  description   = "Service account for CDC Flink projects"
}


# Data source for existing service account
data "confluent_service_account" "cdc_sa" {
  count = var.existing_service_account_id != null ? 1 : 0
  id    = var.existing_service_account_id
}

# Local value to reference service account (created or existing)
locals {
  service_account_id = var.existing_service_account_id != null ? var.existing_service_account_id : confluent_service_account.cdc_sa[0].id
  service_account = var.existing_service_account_id != null ? data.confluent_service_account.cdc_sa[0] : confluent_service_account.cdc_sa[0]
}

# -----------------------------------------------------------------------------
# Service Account Permissions
# -----------------------------------------------------------------------------
# Grant EnvironmentAdmin role to service account for topic management
resource "confluent_role_binding" "cdc_sa_env_admin" {
  principal   = "User:${local.service_account_id}"
  role_name   = "EnvironmentAdmin"
  crn_pattern = local.environment_resource_name

  lifecycle {
    prevent_destroy = false
  }
}

# -----------------------------------------------------------------------------
# API Keys
# -----------------------------------------------------------------------------
# Kafka API Key for service account
resource "confluent_api_key" "cdc_sa_kafka_key" {
  display_name = "${var.prefix}-kafka-api-key-${random_id.env_display_id.hex}"
  description  = "Kafka API Key for service account to manage topics"

  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = local.kafka_cluster_id
    api_version = var.existing_kafka_cluster_id != null ? data.confluent_kafka_cluster.cdc_cluster[0].api_version : confluent_kafka_cluster.cdc_cluster[0].api_version
    kind        = var.existing_kafka_cluster_id != null ? data.confluent_kafka_cluster.cdc_cluster[0].kind : confluent_kafka_cluster.cdc_cluster[0].kind

    environment {
      id = local.environment_id
    }
  }

  depends_on = [
    confluent_role_binding.cdc_sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}

resource "confluent_api_key" "app-manager-schema-registry-api-key" {
  display_name = "${var.prefix}-schema-registry-api-key"
  description  = "Schema Registry API Key that is owned service account"
  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = local.schema_registry_id
    api_version = local.schema_registry.api_version
    kind        = local.schema_registry.kind

    environment {
      id = local.environment_id
    }
  }
  depends_on = [
    confluent_role_binding.cdc_sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}

# Flink API Key for service account
resource "confluent_api_key" "cdc_sa_flink_key" {
  display_name = "${var.prefix}-flink-api-key-${random_id.env_display_id.hex}"
  description  = "Flink API Key for service account to manage Flink statements"

  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = data.confluent_flink_region.cdc_flink_region.id
    api_version = data.confluent_flink_region.cdc_flink_region.api_version
    kind        = data.confluent_flink_region.cdc_flink_region.kind

    environment {
      id = local.environment_id
    }
  }

  depends_on = [
    confluent_role_binding.cdc_sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}

# Tableflow API Key for service account
# Required for enabling Tableflow on topics
resource "confluent_api_key" "cdc_sa_tableflow_key" {
  display_name = "${var.prefix}-tableflow-api-key-${random_id.env_display_id.hex}"
  description  = "Tableflow API Key for service account to enable Tableflow on topics"

  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = "tableflow"
    api_version = "tableflow/v1"
    kind        = "Tableflow"
  }

  depends_on = [
    confluent_role_binding.cdc_sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}





# -----------------------------------------------------------------------------
# Kafka Topics
# -----------------------------------------------------------------------------
# Smoke test topic
resource "confluent_kafka_topic" "smoke_test_records" {
  kafka_cluster {
    id = local.kafka_cluster_id
  }
  topic_name       = "smoke_test_records"
  partitions_count = 1
  rest_endpoint    = local.kafka_cluster_rest_endpoint
  credentials {
    key    = confluent_api_key.cdc_sa_kafka_key.id
    secret = confluent_api_key.cdc_sa_kafka_key.secret
  }

  lifecycle {
    prevent_destroy = false
  }
}
